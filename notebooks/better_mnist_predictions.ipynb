{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAhB0ibxBJBb"
      },
      "source": [
        "# Better MNIST Predictions with PyTorch\n",
        "\n",
        "**It is recommended that you complete this notebook in Google Colab:**\n",
        "- otherwise you may encounter errors\n",
        "- this will allow you to access free GPU resources\n",
        "\n",
        "---\n",
        "In this notebook, we'll take a look at an important limitation of logistic regression and MLP models applied to images, then overcome these limitations by training a shallow CNN in PyTorch.\n",
        "\n",
        "Goals are as follows:\n",
        "\n",
        "- Observe that logistic regression is *not* effective when digits are not centered (why?)\n",
        "- Train a simple CNN to identify digits *even when their position varies*\n",
        "- Visualize the filters for each digit learned by the CNN\n",
        "- Create and train a model that classifies handwritten digits with over 98% accuracy\n",
        "\n",
        "We'll begin by importing required libraries:\n",
        "\n",
        "- numpy for efficient math operations\n",
        "- sklearn for defining and training our logistic regression and MLP models\n",
        "- matplotlib for visualization/plotting\n",
        "- torch modules for neural networks, optimization functions, activation functions, and creating baches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ez69LmdZBJBd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxWCcVu_BJBd"
      },
      "source": [
        "As before, we'll load the MNIST dataset. The data are already broken down into:\n",
        "\n",
        "1. a development set, which we'll use for training\n",
        "2. a test set, which we'll use to evaluate performance\n",
        "\n",
        "As in the previous exercise, we will not be tuning our models, so we will not set aside a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0c4OWUGBJBe",
        "outputId": "29ee1c2f-841e-4c2d-9545-cc61c5280eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 489kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.45MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.26MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "There are 60000 training images.\n",
            "There are 10000 test images.\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load MNIST\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "# Extract data and labels (seperate images and labels)\n",
        "x_dev = train_dataset.data  # Shape: (60000, 28, 28)\n",
        "y_dev = train_dataset.targets  # Shape: (60000,)\n",
        "\n",
        "x_test = test_dataset.data  # Shape: (10000, 28, 28)\n",
        "y_test = test_dataset.targets  # Shape: (60000,)\n",
        "\n",
        "\n",
        "\n",
        "# Normalize values so that they range from 0 to 1\n",
        "x_dev = x_dev / 255.\n",
        "x_test = x_test / 255.\n",
        "\n",
        "print('There are', len(x_dev), 'training images.')\n",
        "print('There are', len(x_test), 'test images.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by1ydij9BJBe"
      },
      "source": [
        "To review code that allows us to inspect the images and labels, please review [Computational Exercise 5](https://github.com/mengelhard/bsrt_ml4h/blob/master/notebooks/ce5.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqIbwuLQBJBj"
      },
      "source": [
        "## Un-Centering our Digits\n",
        "In MNIST, all digits are centered, which substantially simplifies the problem. In many real-world datasets, on the other hand, we need to identify image features regardless of where they may be present within the image. Let's explore this issue by modifying MNIST so that digits are no longer centered. We'll first enlarge the images, then place digits at random positions within the enlarged image. We'll do this by padding each image (in both our development and test sets) with zeroes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VXi4BkrBBJBj"
      },
      "outputs": [],
      "source": [
        "def pad_image(img, pad_len):\n",
        "    m, n = img.shape\n",
        "    i = np.random.randint(pad_len - 2) + 1 # pick a horizontal offset between 1 and (pad_len - 1)\n",
        "    j = np.random.randint(pad_len - 2) + 1 # pick a vertical offset bewteen 1 and (pad_len - 1)\n",
        "    img = np.concatenate([[[0] * i] * m, img, [[0] * (pad_len - i)] * m], axis=1) # pad horizontally\n",
        "    img = np.concatenate([[[0] * (n + pad_len)] * j, img, [[0] * (n + pad_len)] * (pad_len - j)], axis=0) # pad vertically\n",
        "    return img\n",
        "\n",
        "x_dev_padded = np.array([pad_image(x, 20) for x in x_dev])\n",
        "x_test_padded = np.array([pad_image(x, 20) for x in x_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAQ6sQywBJBk"
      },
      "source": [
        "In the plots below, we see that digits are no longer centered in the modified dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "-FbbcU9tBJBk",
        "outputId": "1a793d7f-4a97-4086-e838-efe23f128850"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
        "for i in range(5):\n",
        "    ax[i].imshow(x_dev_padded[i, :, :], cmap='gray')\n",
        "    ax[i].set_title('MNIST Image %i, Label = %i' % (i, y_dev[i]), fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqjCG14RBJBk"
      },
      "source": [
        "## Exercise 1: Logistic Regression\n",
        "Let's see whether logistic regression is able to classify digits effectively now that they're not centered. In the following block, you should:\n",
        "- Train a logistic regression model (i.e. LogisticRegression()) to predict the labels of digits in MNIST.\n",
        "- After training a model on the development set, evaluate its accuracy on the test set. As in the previous computational excercise, you'll need to flatten the images in order to use logistic regression. Remember that your predictions are no longer binary; instead, you are predicting one of ten different digits.\n",
        "- Visualize the logistic regression filters using the same or similar code as in [computational exercise 5](https://github.com/mengelhard/bsrt_ml4h/blob/master/notebooks/ce5.ipynb). Note that since we've padded the images, the filters are now (48, 48) in size rather than (28, 28)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVLbmiJLBJBk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "### CREATE A LOGISTIC REGRESSION MODEL AND TRAIN IT ON THE FLATTENED DEVELOPMENT SET ###\n",
        "\n",
        "\n",
        "### USE THE TRAINED MODEL TO MAKE PREDICTIONS ON THE (FLATTENED) TEST SET ###\n",
        "\n",
        "\n",
        "### EVALUATE THE ACCURACY OF THE MODEL'S PREDICTIONS ON THE TEST SET ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVuPpQcmBJBl"
      },
      "outputs": [],
      "source": [
        "### ACCESS THE MODEL COEFFICIENTS ###\n",
        "\n",
        "\n",
        "### RESHAPE EACH OF THE 10 COEFFICIENT VECTORS TO HAVE SHAPE (48, 48) ###\n",
        "\n",
        "\n",
        "### PLOT EACH OF THE RESHAPED VECTORS (i.e. FILTERS) ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld62tsVEBJBl"
      },
      "source": [
        "## Our First Pytorch Model\n",
        "---\n",
        "Lets create a simple CNN, thats has the following properties.\n",
        "1. **10 Filters**: one for each digit each of size 28 x 28 (same size as each unmodified MNIST image).\n",
        "2. The CNN we are creating is unusual and inefficient due to the small number of filters, where each are relatively large in size.\n",
        "3. We will max pool the output of each filter across each image. We want to pool the filters, so we can see if the digit corresponding to that filter is present in the whole image.\n",
        "   - The output of the **Pooling Operation** will give us the predicted **log-odds** for each of the 10 digits.\n",
        "4. **Steps**\n",
        "   1. Convert our `MNIST` image dataset into Tensors, so the images can be passed into the `PyTorch` Model.\n",
        "   2. Split training, and test datasets into batches. We will use a batch size of 32, which means that the model will process 32 images at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wBCbw3mPCl2N"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# convert training and test datasets to tensor\n",
        "train_ds = TensorDataset(\n",
        "    torch.tensor(x_dev_padded, dtype=torch.float32), # features\n",
        "    y_dev.to(dtype=torch.long)\n",
        ")\n",
        "test_ds = TensorDataset(\n",
        "    torch.tensor(x_test_padded, dtype=torch.float32),\n",
        "    y_test.to(dtype=torch.long)\n",
        ")\n",
        "\n",
        "# create batch size of 32\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm54817ZXiLO"
      },
      "source": [
        "#### Code Explanation\n",
        "\n",
        "We first convert the training (`x_dev_padded` and `y_dev`) and test (`x_test_padded` and `y_test`) into **Tensors** using `TensorDataset` from the `torch.utils.data` module.\n",
        "\n",
        "We then use the tensor objects (`train_ds` and `test_ds`) to create data loaders with a batch size of  32 (`train_loader` and `test_loader`). This is done using PyTorch's `DataLoader` class, which handles batches and other features like shuffling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76RDAeWtgEMS"
      },
      "source": [
        "We can then loop over our batches as shown in the next block. Here our goal is just to see what a single batch looks like, so we'll break the loop after printing out the first batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkSxcoqICsHN",
        "outputId": "97c351bc-b132-4394-f8ac-beb1107cd62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The images have shape (32, 48, 48)\n",
            "The labels have shape (32,)\n"
          ]
        }
      ],
      "source": [
        "for images, labels in train_loader:\n",
        "    print('The images have shape', images.numpy().shape)\n",
        "    print('The labels have shape', labels.numpy().shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYqZNJU4hJWd"
      },
      "source": [
        "## Defining the Model\n",
        "\n",
        "To create the model, we need to make different types of layers (dense layers, convolutional layers, max pool layers, flatten layers).\n",
        "\n",
        "1. `Conv2d`: Convolutional layers, with:\n",
        "   - `in_channels`=1, refers to grayscale images\n",
        "   - `out_channels`=10, refers to the 10 features (numbers 0 to 9)\n",
        "   - `kernel_size`=28, refers to filter size 28x28\n",
        "   - `bias`=false, means no bias term is used in the `Conv2d` layer\n",
        "2. `nn.MaxPool2d`: This is the max pooling layer\n",
        "   - `kernel_size`=21, size of pooling window is 21x21\n",
        "3. `nn.Flatten()`: flattening layer; after applying convolution and pooling we must turn the multi-dimension tensor into a 1D vector\n",
        "\n",
        "To make this model we are using `Module` from `torch.nn`. The forward method defines operation to apply to the input tensor `x`.\n",
        "- `x.unsqueeze(1)`: Adds a channel dimension to `x`\n",
        "- `self.conv1`: Aplies the convolutional layer to `x`\n",
        "- `self.mp1`: passes feature maps through max pooling layer, to find maximum value within 21x21\n",
        "- `self.flatten` flattens the multidimensional tensor into a 1D vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dXS8_qFSBJBl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=10): #in_channel refers to color, but for MNIST its always 1\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels=1, out_channels=10, kernel_size=28, bias=False)\n",
        "        self.mp1=nn.MaxPool2d(kernel_size=21)\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=x.unsqueeze(1)\n",
        "        x=self.conv1(x)\n",
        "        x=self.mp1(x)\n",
        "        x=self.flatten(x)\n",
        "\n",
        "        return(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJYOs_fQlv7K"
      },
      "source": [
        "We can then create an instance of our `SimpleCNN`, just as we did when using `LogisticRegression` or `MLPClassifier` in `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1JT6n8ZjTfSD"
      },
      "outputs": [],
      "source": [
        "model = SimpleCNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f6jkLM2l_Ee"
      },
      "source": [
        "Before training our model, we must first create **loss** and **optimization** objects\n",
        "- We will create the `loss_object` to calculate the cross-entropy loss based on two factors\n",
        "  1. Model's predicted logits\n",
        "  2. True labels in training set\n",
        "- We will create the `optimizer` object to determine model's parameters based on Adam optimizer (modified form of stochastic gradient descent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QotAGwTRTd3S"
      },
      "outputs": [],
      "source": [
        "loss_object = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5PRlxAvBJBm"
      },
      "source": [
        "We will now train our model `model`, by looping over all batches in training set by the number of epochs. Epochs are a full pass through the entire training dataset. We'll loop over all the batches in our training set 5 times in a row (i.e. 5 EPOCHS). For each batch, we'll:\n",
        "1. apply our model to generate predictions for that batch\n",
        "2. calculate the loss based on those predictions\n",
        "3. calculate the gradient of the loss (i.e. how much does loss change in respose to changes in each parameter)\n",
        "4. use our optimizer to adjust the model parameters (i.e. `model.trainable_variables`) based on the gradient.\n",
        "5. Finally we will calculate the accuracy for each epoch\n",
        "**Important**: This code will be very slow unless you switch to a GPU runtime from the Colab menu. Even with this change, it'll still take a minute or two.\n",
        "- Runtime > Change runtime type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvYkMdHwBJBn",
        "outputId": "3671f366-2d92-48c9-f562-881c720b184f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train accuracy = 85.9%, test accuracy = 91.7%\n",
            "Epoch 2: train accuracy = 92.3%, test accuracy = 93.2%\n",
            "Epoch 3: train accuracy = 93.5%, test accuracy = 94.0%\n",
            "Epoch 4: train accuracy = 94.2%, test accuracy = 94.3%\n",
            "Epoch 5: train accuracy = 94.6%, test accuracy = 94.5%\n"
          ]
        }
      ],
      "source": [
        "num_epochs=5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_accuracy = []\n",
        "    test_accuracy = []\n",
        "\n",
        "    model.train()\n",
        "# training loop\n",
        "    for images, labels in train_loader:\n",
        "        predicted_logits = model(images)\n",
        "        loss = loss_object(predicted_logits, labels)\n",
        "\n",
        "# backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Calculate accuracy\n",
        "        _, predicted_labels = torch.max(predicted_logits, 1)  # Get predicted class indices\n",
        "        batch_accuracy = (predicted_labels == labels).float().mean().item()  # Mean accuracy for batch\n",
        "        train_accuracy.append(batch_accuracy)\n",
        "\n",
        "# evaluation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
        "        for images, labels in test_loader:\n",
        "            # Forward pass\n",
        "            predicted_logits = model(images)\n",
        "\n",
        "            # Calculate test accuracy\n",
        "            _, predicted_labels = torch.max(predicted_logits, 1)\n",
        "            batch_accuracy = (predicted_labels == labels).float().mean().item()\n",
        "            test_accuracy.append(batch_accuracy)\n",
        "\n",
        "    # calculate accuracy\n",
        "    train_accuracy = 100 * np.mean(train_accuracy)\n",
        "    test_accuracy = 100 * np.mean(test_accuracy)\n",
        "    print(f'Epoch {epoch + 1}: train accuracy = {train_accuracy:.1f}%, test accuracy = {test_accuracy:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2S9_qsVpmkh"
      },
      "source": [
        "## Exercise 2: Visualize the filters learned by the CNN\n",
        "The filters of the trained model can be accessed as an attribute of the `model.conv1.weight` layer, as shown in the first line of the block below. `filters` will be an array of size (10, 28, 28) corresponding to the 10 filters -- one for each digit -- each of which is the same size (28 by 28) as the images. In the following block, you should plot and inspect these filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj-cTTKEbhzd"
      },
      "outputs": [],
      "source": [
        "filters = model.conv1.weight.data.numpy()\n",
        "filters = (filters - filters.min()) / (filters.max() - filters.min()) # normalize values\n",
        "\n",
        "filters = filters.squeeze(1)\n",
        "### PLOT EACH OF THE 10 28 by 28 FILTERS ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOiUVhscBJBp"
      },
      "source": [
        "## Exercise 3: Better CNN\n",
        "**Note: This exercise is optional for those not already familar with Python**\n",
        "\n",
        "The CNN we created above is inefficient due to the large size of each filter (28x28).\n",
        "\n",
        "In this exercise, you'll build a better CNN, train it, and evaluate it.\n",
        "- Create the model by following the prompts in the code block below.\n",
        "- Then, duplicate code blocks above to create an instance of `BetterCNN`, train it, and evaluate its performance on the test set.\n",
        "\n",
        "The following may also be interesting, though none is required:\n",
        "- visualize the learned filters in the convolutional layer.\n",
        "- modify the architecture to see if performance can be further improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swp_ZOQ3BJBr"
      },
      "source": [
        "The following blocks, repeated from above, can now be used to train and evaluate `BetterCNN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPv0Ve2rboyC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BetterCNN(Model):\n",
        "    def __init__(self):\n",
        "        super(BetterCNN, self).__init__()\n",
        "        ### DEFINE A CONVOLUTIONAL LAYER WITH 32 FILTERS OF SIZE 3 (i.e. 3x3) AND input size of 1 ###\n",
        "        ### DEFINE A MAX POOL LAYER WITH 2x2 POOL SIZE ###\n",
        "        ### DEFINE A SECOND CONVOLUTIONAL LAYER WITH 32 FILTERS OF SIZE 5 AND RELU ACTIVATION ###\n",
        "        ### DEFINE A FLATTEN LAYER ###\n",
        "        ### DEFINE A DENSE LAYER WITH OUTPUT SIZE 128  ###\n",
        "        ### DEFINE A DENSE LAYER WITH OUTPUT SIZE 10. THIS IS THE FINAL LAYER; OUTPUT WILL BE THE LOG-ODDS ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        #x = ### APPLY FIRST CONVOLUTIONAL LAYER ###\n",
        "        #x = ### APPLY MAX POOL LAYER ###\n",
        "        #x = ### APPLY SECOND CONVOLUTIONAL LAYER ###\n",
        "        #x = ### APPLY MAX POOL LAYER ###\n",
        "        #x = ### APPLY FLATTEN LAYER ###\n",
        "        #x = ### APPLY FIRST DENSE LAYER ###\n",
        "        #x = ### APPLY SECOND DENSE LAYER ###\n",
        "        return x\n",
        "\n",
        "\n",
        "### SIDE NOTE: I recommend you apply the F.relu() method to the Conv2D, and  Dense Layers for better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxeo7qKiykwr"
      },
      "outputs": [],
      "source": [
        "model = BetterCNN()\n",
        "loss_object = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hl4O2hJiRCMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqfuSPXYviRJ",
        "outputId": "b772a97a-2cce-4dba-945d-09e40f21d060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train accuracy = 86.0%, test accuracy = 95.2%\n",
            "Epoch 2: train accuracy = 95.6%, test accuracy = 95.6%\n",
            "Epoch 3: train accuracy = 97.3%, test accuracy = 95.6%\n",
            "Epoch 4: train accuracy = 98.1%, test accuracy = 95.0%\n",
            "Epoch 5: train accuracy = 98.6%, test accuracy = 96.9%\n"
          ]
        }
      ],
      "source": [
        "num_epochs=5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_accuracy = []\n",
        "    test_accuracy = []\n",
        "\n",
        "    model.train()\n",
        "# training loop\n",
        "    for images, labels in train_loader:\n",
        "        predicted_logits = model(images)\n",
        "        loss = loss_object(predicted_logits, labels)\n",
        "\n",
        "# backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Calculate accuracy\n",
        "        _, predicted_labels = torch.max(predicted_logits, 1)  # Get predicted class indices\n",
        "        batch_accuracy = (predicted_labels == labels).float().mean().item()  # Mean accuracy for batch\n",
        "        train_accuracy.append(batch_accuracy)\n",
        "\n",
        "# evaluation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
        "        for images, labels in test_loader:\n",
        "            # Forward pass\n",
        "            predicted_logits = model(images)\n",
        "\n",
        "            # Calculate test accuracy\n",
        "            _, predicted_labels = torch.max(predicted_logits, 1)\n",
        "            batch_accuracy = (predicted_labels == labels).float().mean().item()\n",
        "            test_accuracy.append(batch_accuracy)\n",
        "\n",
        "    # calculate accuracy\n",
        "    train_accuracy = 100 * np.mean(train_accuracy)\n",
        "    test_accuracy = 100 * np.mean(test_accuracy)\n",
        "    print(f'Epoch {epoch + 1}: train accuracy = {train_accuracy:.1f}%, test accuracy = {test_accuracy:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iesZQUopBJBs"
      },
      "source": [
        "## Steps to distribute your work as an html file:\n",
        "\n",
        "If you're using Anaconda on your local machine:\n",
        "- download your notebook as html (see `File > Download as > HTML (.html)`)\n",
        "\n",
        "If you're using Google Colab:\n",
        "- download your notebook as .ipynb (see `File > Download > Download .ipynb`)\n",
        "- if you have nbconvert installed, convert it to .html and submit it in Talent LMS\n",
        "- if not, you may need to place the .ipynb file in a .zip archive in order to distribute it (e.g. to upload or send via email)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}