{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqj6lIUuT75C"
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "**It is recommended that you complete this notebook in Google Colab:**\n",
    "- otherwise you may encounter errors\n",
    "- this will allow you to access free GPU resources\n",
    "\n",
    "---\n",
    "In this notebook, we'll use transfer learning to train the CNN, **Inception v3** to classify images of flowers. A very similar process was used by Esteva et al. to classify skin lesions.\n",
    "\n",
    "**This notebook should be completed using a GPU runtime in Colab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wdHPqDXqBKPL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "# torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if GPU is is being used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zul0DOXnWDgW"
   },
   "source": [
    "Let's begin by using `torchvision` to load Inception v3.\n",
    "\n",
    "**More Info on Inception v3**: Can be found in `torchvision` documentation (https://pytorch.org/vision/stable/models/generated/torchvision.models.inception_v3.html)\n",
    "\n",
    "**Important Note**: This is a *different* version of Inception v3 compared to the one we used in an earlier activity. In this version, the final layer (i.e. the *classification head*) has been removed.\n",
    "\n",
    "We are removing the fully connected (classification) layer because we are only interested in **feature extraction**. We remove the fully connected layer by turning the fully connected layer in `inception_v3` into a identity matrix. By changing the fully connected layer to a identity matrix, the layer now has no effect on the model. This results in the model lacking the fully connected layer. \n",
    "\n",
    "Additionally, we need to remove the **auxillary classifiers**. The purpose of the auxillary classifier is to push useful gradients to lower layers of the model, to improve convergence during training. However, for our example of `inception_v3` we wish to freeze the model, so auxillary classifiers are not needed. The code below disables all the auxillary classifiers (even for training), and removes the auxillary logits from the model so they can't be accidentally turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KtDXkJUkCIOU"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# load the module\n",
    "inception_v3 = models.inception_v3(pretrained=True)\n",
    "\n",
    "# freeze parameters\n",
    "for param in inception_v3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# modify model to output features, instead of predictions\n",
    "inception_v3.fc = nn.Identity() # remove fully connected layer\n",
    "\n",
    "# remove auxillary classifier\n",
    "if inception_v3.aux_logits:\n",
    "    inception_v3.aux_logits = False\n",
    "    inception_v3.AuxLogits = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS7aJdTvnP_X"
   },
   "source": [
    "## Part 1: Identifying Flowers\n",
    "\n",
    "In the first half of this activity, we'll re-train the final layer of Inception v3 to identify several types of flowers. We begin by downloading the dataset as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iHeXWq_FFqs4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is ready at ./flower_photos\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "\n",
    "# download path\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "download_path = './flower_photos.tgz'\n",
    "extract_path = './flower_photos' # create folder flower_photos in working directory\n",
    "\n",
    "# download data\n",
    "if not os.path.exists(download_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, download_path)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "\n",
    "# extract data\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open(download_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset is ready at\", extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFjPpNwWoZUG"
   },
   "source": [
    "Now we can load the dataset from the created folder `flower_photos`.\n",
    "1. Load all training images into batches of size 32, with their associated labels.The labels are determined by the directory structure in the archive we downloaded.\n",
    "2. Resize all images to (299,299) as required by `Inception v3`.\n",
    "3. Convert all images and labels to **tensors**\n",
    "4. Divide the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1cm93gfFqs5",
    "outputId": "e348917f-8cee-49ec-e2ee-91a749ad59b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "# load data\n",
    "flower_root='./flower_photos/flower_photos'\n",
    "\n",
    "# setup classes\n",
    "FLOWER_CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "\n",
    "\n",
    "INPUT_SIZE = 299\n",
    "\n",
    "\n",
    "# create transformation (for image size, and tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=flower_root, transform=transform)\n",
    "\n",
    "# check class names\n",
    "class_names = dataset.classes\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a 80/20 training and test split. In each dataset the batch size will be 32.\n",
    "- We do not need to normalize our data, because it happens automatically when using `transforms.ToTensor()` in the `transform` method above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 3670\n",
      "Training set size: 2936\n",
      "Test set size: 734\n",
      "\n",
      "Total number of batches in train_loader: 92\n",
      "Total number of batches in test_loader: 23\n"
     ]
    }
   ],
   "source": [
    "# create training/test split\n",
    "total_size = len(dataset)\n",
    "test_size = int(total_size *.2)\n",
    "train_size = total_size - test_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# create batch size of 32\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "# check batches\n",
    "\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "print(f\"Training set size: {train_size}\")\n",
    "print(f\"Test set size: {test_size}\")\n",
    "print(f\"\\nTotal number of batches in train_loader: {math.ceil(train_size / 32)}\")\n",
    "print(f\"Total number of batches in test_loader: {math.ceil(test_size / 32)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTo5dol9sn9X"
   },
   "source": [
    "## Exercise 8.1: Visualize the Flowers Dataset\n",
    "\n",
    "Similar to previous exercises, we can use `plt.imshow` to plot images in a batch generated by our `train_loader` Dataset. In the following block, you should plot and label these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "TC_RYB7EFqs5",
    "outputId": "3316079d-b707-43ea-c043-489ceedabbf8"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
    "\n",
    "# Get a single batch from the train dataset\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    # We must change the image from (Channels, Height, Width) to (Height, Width, Channels), and convert it to a NumPy array.\n",
    "    image = images[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "    # PLOT THE IMAGES\n",
    "    \n",
    "\n",
    "    # SET THE TITLE FOR THE IMAGE WITH THE CORRESPONDING LABEL\n",
    "    \n",
    "\n",
    "    # Turn off the axis for a cleaner image\n",
    "    ax[i].axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9yzSm1lv7Xb"
   },
   "source": [
    "We're now ready to create our prediction model following the steps described in our [medical image analysis lecture](https://github.com/mengelhard/bsrt_ml4h/blob/master/lectures/ll3.pdf):\n",
    "- Start with our Inception v3 feature extractor. We have already removed the classification head (final connected layer) above, but we like to add a new fully connected layer.\n",
    "-  The new fully connected layer will be used to predict the image label (flower type) based on the extracted features. We will be learning the weights in this new layer.\n",
    "- We can now use `summary()` to see a description of our model. It is very long, because we can see each individual layer of the `inception_v3` model.\n",
    "---\n",
    "**In the Code Below**:\n",
    "1. We first create the class `FeatureExtractorWrapper` using the library torch.nn. This`FeatureExtractorWrapper` class is used as a wrapper for our InceptionV3 model. We use the wrapper because we wish to isolate feature extraction. It takes the `inception_v3` model we created as an input and outputs the model after applying the feature extractor wrapper.\n",
    "   \n",
    "3. We next create the class `CustomInceptionV3`. This class takes the input of the `feature_extractor`, and number of classes and outputs the new model, after adding a new fully connected layer. The new fully connected layer takes an input of 2048 features, and outputs a feature for each class for our data set. By adding this fully connected layer, we can  predict the flower type for each flower image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqHQ30SDHQSf",
    "outputId": "34085830-03d2-4e9d-df1a-41dd5201ee66"
   },
   "outputs": [],
   "source": [
    "# feature extractor wrapper to extract features using InceptionV3\n",
    "class FeatureExtractorWrapper(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(FeatureExtractorWrapper, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "\n",
    "# Custom Model with Feature Extraction and Classification\n",
    "class CustomInceptionV3(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes):\n",
    "        super(CustomInceptionV3, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.classifier = nn.Linear(2048, num_classes)  # Final dense layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction step\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)  # Final classification layer\n",
    "        return x\n",
    "\n",
    "# apply feature wrapper on inceptionv3\n",
    "feature_extractor = FeatureExtractorWrapper(inception_v3)\n",
    "\n",
    "num_classes = len(FLOWER_CLASSES)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = CustomInceptionV3(feature_extractor, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model, its helpful to check the total parameters, trainable parameters, and non-trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJLlY24ezfhH"
   },
   "source": [
    "To begin training, we'll first need to define our loss, and our optimizer, just as we did in our previous computational exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMEWOgXVHQdK"
   },
   "outputs": [],
   "source": [
    "loss_object = nn.CrossEntropyLoss() # multi-class cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters()) #modified stochastic gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpVNPNg5mNVM"
   },
   "source": [
    "## Exercise 8.2: Train the Flowers Model and Evaluate Performance\n",
    "\n",
    "We're now ready to train our model, which we can do with the following block. Again, this is identical to code we used in the previous exercise. However, this time you should **make the following adjustments**:\n",
    "- Try training for more epochs\n",
    "- Add code to monitor accuracy on the training set and test set after each epoch\n",
    "- After the training process has completed, plot training and test performance by epoch. Is your model overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCaArLeEHQj6",
    "outputId": "44c63d6a-09e9-4cc1-b8b6-dcf8cd6afc1b"
   },
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "# TRAINING LOOP\n",
    "    model.train() \n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions= model(images)\n",
    "        loss=loss_object(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # CALCULATE TRAINING ACCURACY\n",
    "\n",
    "\n",
    "# EVALUATION LOOP\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for images, labels in test_loader:\n",
    "            # RUN FORWARD PASS FOR TEST DATA\n",
    "            # CALCULATE TEST ACCURACY\n",
    "   \n",
    "    # PRINT ACCURACY FOR EACH EPOCH FOR BOTH TEST AND TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_v0m6uiz9UI"
   },
   "source": [
    "## Exercise 8.3: Make Predictions and Inspect Them\n",
    "\n",
    "We're done training! We've already taken a look at the accuracy, but let's also inspect a few images in the test set. In the following block, you should plot at least 5 test images along with (a) the predicted label, and (b) the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "KzT_h_eaHQoU",
    "outputId": "e0e4ae6f-3d86-4932-806a-59ffa3395dc1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "## ADD CODE TO PREDICT THE LABELS FOR THIS BATCH OF TEST IMAGES ##\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "  ## ADD CODE TO PLOT THE IMAGE ##\n",
    "\n",
    "\n",
    "  ## ADD CODE TO TITLE THE IMAGE WITH THE PREDICTED AND TRUE LABELS ##\n",
    "  \n",
    "  \n",
    "  # turn off the axes to make the images look nicer\n",
    "  ax[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhoN78yqdQOd"
   },
   "source": [
    "## Part 2: Mammograms\n",
    "\n",
    "In the second half of this activity, we'll re-train the final layer of Inception v3 to identify types of breast tissue from [the mini-MIAS database of mammograms](http://peipa.essex.ac.uk/info/mias.html). This dataset is very small, which makes it easy to work with in a short amount of time. While we're unlikely to get good accuracy with a dataset of this size, the same process could be used to obtain good performance on a larger dataset. We'll begin by downloading the data the same way we downloaded the flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5c-jvu03-7O"
   },
   "outputs": [],
   "source": [
    "# download path\n",
    "url = 'http://peipa.essex.ac.uk/pix/mias/all-mias.tar.gz'\n",
    "download_path = 'all-mias.tar.gz'\n",
    "extract_path = './mias_dataset'\n",
    "\n",
    "# download data\n",
    "if not os.path.exists(download_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, download_path)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "\n",
    "# extract data\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open(download_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset is ready at\", extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3werxg12CRO"
   },
   "source": [
    "**Now lets load the dataset**\n",
    "\n",
    "We can now build our dataset from the downloaded files. This time, we'll build the dataset manually instead of using a function. This will make it easier to see all the individual steps, but will use more RAM than we did before, because all the images will be stored in memory. Steps include:\n",
    "- Load all images in the directory we downloaded using `cv2`.\n",
    "- Resize all images to (299, 299) and rescale their pixels to range from 0 to 1, as required for Inception v3.\n",
    "- Convert the images and labels to PyTorch tensors ready to be fed through a PyTorch graph.\n",
    "- Use pandas to read a table containing information corresponding to each image, including the tissue type and severity of any abnormalities. We will be predicting tissue type rather than severity, because the latter only exists for images that contain abnormalities.\n",
    "- Divide the data into training and test sets.\n",
    "\n",
    "**The details of the code in this block are not important, but it is important that you understand why these steps above are needed.**\n",
    "\n",
    "After running this block, you may want to take a look at `mias_df`, which contains information about each image, as well as the shape of `mias_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zad_FQFq5lNu",
    "outputId": "910ba8a7-ee74-4f19-dc73-c5073200268c"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import cv2\n",
    "mias_root='./mias_dataset/mias_dataset'\n",
    "\n",
    "INPUT_SIZE = 299\n",
    "def read_image(fn, size=(INPUT_SIZE, INPUT_SIZE)):\n",
    "  img = cv2.imread(fn)[:, :, ::-1] # Read in image, and convert from BGR(blue, green, red) to RGB(red, green, blue)\n",
    "  img = cv2.resize(img, size) # Resize image to 299*299\n",
    "  return img / 255. # normalize each image to range [0,1]\n",
    "\n",
    "mias_df = pd.read_table(\n",
    "    os.path.join(os.path.dirname(mias_root), 'Info.txt'),\n",
    "    sep=' ',\n",
    "    skiprows=101,\n",
    "    skipfooter=2,\n",
    "    header=None,\n",
    "    engine='python',\n",
    "    names=['id', 'tissue', 'type', 'severity', 'x_coord', 'y_coord', 'radius']\n",
    ")[['id', 'tissue', 'type']].drop_duplicates().sample(frac=1., random_state=2021) # drop all columns besides ['id', 'tissue', 'type']\n",
    "\n",
    "mias_filenames = os.listdir(os.path.dirname(mias_root))\n",
    "mias_ids = [fn.split('.')[0] for fn in mias_filenames]\n",
    "mias_df = mias_df[mias_df['id'].isin(mias_ids)]\n",
    "\n",
    "mias_labels, mias_classes = pd.factorize(mias_df['tissue'])\n",
    "mias_images = np.array([read_image(os.path.join(os.path.dirname(mias_root), str(id) + '.pgm')) for id in mias_df['id']])\n",
    "\n",
    "batch_size = 23\n",
    "split_point = 23 * 12 #split the data based on the 276 sample\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_images, test_images = mias_images[:split_point], mias_images[split_point:] # first 276 images go into training, rest go into test\n",
    "train_labels, test_labels = mias_labels[:split_point], mias_labels[split_point:]\n",
    "\n",
    "\n",
    "# Convert Images and Labels to PyTorch Tensors\n",
    "train_images = torch.tensor(train_images, dtype=torch.float32).permute(0, 3, 1, 2)  # Change shape of the tensor to (Number, Channels, Height, Width)\n",
    "test_images = torch.tensor(test_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# check shape  \n",
    "for images, labels in train_loader:\n",
    "    batch_size, channels, height, width = images.shape  # Unpack the shape\n",
    "    label_size=labels.shape[0]\n",
    "    print(f\"Shape of Images\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Channels: {channels}\")\n",
    "    print(f\"Height: {height}\")\n",
    "    print(f\"Width: {width}\")\n",
    "    print(f\"\\nShape of Labels\")\n",
    "    print(f\"Label Size: {label_size}\")\n",
    "    break  # Exit after the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQnx8gjW4Y6k"
   },
   "source": [
    "## Exercise 8.4: Visualize the mini-MIAS Dataset\n",
    "\n",
    "This time, even though we did create Pytorch Tensor Datasets to use in training (`train_loader` and `test_loader`), we can also access the images and labels directly in `mias_images` and `mias_labels`, and `mias_classes` tells us the tissue type corresponding to each label. In the following block, you should plot and label at least 5 images from mini-MIAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "OhrO7loJ-Vlm",
    "outputId": "e003c3af-05af-46a5-c586-8afd13b4d37c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "  ## ADD CODE TO PLOT THE IMAGE ##\n",
    "\n",
    "\n",
    "  ## ADD CODE TO TITLE THE IMAGE WITH THE CORRESPONDING LABEL ##\n",
    "  \n",
    "  \n",
    "  # turn off the axes to make the images look nice\n",
    "  ax[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOOb_auH5oKk"
   },
   "source": [
    "Once again, we're ready to create our prediction model following the steps described in our [medical image analysis lecture](https://github.com/mengelhard/bsrt_ml4h/blob/master/lectures/ll3.pdf):\n",
    "- Start with our Inception v3 feature extractor. The classification head (i.e. final layer) has already been removed, but we do need to convert this from a Tensorflow Hub model to a Layer object that can be incorporated in our new, flower prediction model.\n",
    "- We'll stick with `trainable=False`. It may be interesting to try `trainable=True`, but it's unlikely to work well with such a small dataset. If we had a large enough dataset to support this, we'd need a lot more time.\n",
    "- Add a Dense (i.e. fully-connected) layer that will predict the image label (i.e. tissue type) based on the extracted features. We will be learning the weights in this new layer.\n",
    "\n",
    "We can now use `model.summary()` to see a description of our model. Again, our single `inception_v3` is actually the entire Inception v3 CNN, which is why it contains so many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uK4sLDSePpdp",
    "outputId": "041f18c9-6a29-4672-cfab-48f6ea813699"
   },
   "outputs": [],
   "source": [
    "num_classes=len(mias_classes)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    inception_v3,\n",
    "    nn.Linear(in_features=2048, out_features=num_classes)\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKqij8dy6IHi"
   },
   "source": [
    "We'll use the same loss and optimizer as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlHK52JcT9m1"
   },
   "outputs": [],
   "source": [
    "loss_object = nn.CrossEntropyLoss() # multi-class cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters()) #modified stochastic gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CS4jaJmx6ZF8"
   },
   "source": [
    "## Exercise 8.5: Train the MIAS Model and Evaluate Performance\n",
    "\n",
    "Now we're ready to start training! You should modify the code below the same way you did before:\n",
    "- Evaluate accuracy on the training set and test set after each epoch\n",
    "- Additionally, it may be helpful to evaluate performance on the test set before we start training so we know what our starting point is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "# TRAINING LOOP\n",
    "    model.train() \n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions= model(images)\n",
    "        loss=loss_object(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # CALCULATE TRAINING ACCURACY\n",
    "\n",
    "\n",
    "# EVALUATION LOOP\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for images, labels in test_loader:\n",
    "            # RUN FORWARD PASS FOR TEST DATA\n",
    "            # CALCULATE TEST ACCURACY\n",
    "   \n",
    "    # PRINT ACCURACY FOR EACH EPOCH FOR BOTH TEST AND TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C5x60rz7AGK"
   },
   "source": [
    "When training a model like this, in addition to the fact that you'll typically be working with a much larger dataset, there are a few additional factors to consider, including:\n",
    "- The depth to which you'll fine-tune. To adjust this, we'd need code to set individual layers within Inception v3 to trainable or not, rather than Inception v3 as a whole.\n",
    "-  Data augmentation, which typically includes random crops, rotations, and other distortions.\n",
    "\n",
    "Still, this code provides the general road map you'd need to make predictions on your own dataset of medical images.\n",
    "\n",
    "## Steps to distribute your work as an html file:\n",
    "\n",
    "If you're using Anaconda on your local machine:\n",
    "- download your notebook as html (see `File > Download as > HTML (.html)`)\n",
    "\n",
    "If you're using Google Colab:\n",
    "- download your notebook as .ipynb (see `File > Download > Download .ipynb`)\n",
    "- if you have nbconvert installed, convert it to .html and submit it in Talent LMS\n",
    "- if not, you may need to place the .ipynb file in a .zip archive in order to distribute it (e.g. to upload or send via email)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ce7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
